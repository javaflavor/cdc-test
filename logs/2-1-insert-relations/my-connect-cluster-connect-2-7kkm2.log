Starting Kafka Connect with custom plugin directory /tmp/kafka-plugins
Preparing truststore
Certificate was added to keystore
Preparing truststore is complete
Starting Kafka Connect with configuration:
# Bootstrap servers
bootstrap.servers=my-cluster-kafka-bootstrap:9093
# REST Listeners
rest.port=8083
rest.advertised.host.name=10.129.2.184 
rest.advertised.port=8083
# Plugins
plugin.path=/tmp/kafka-plugins
# Provided configuration
offset.storage.topic=connect-cluster-offsets
value.converter=org.apache.kafka.connect.json.JsonConverter
config.storage.topic=connect-cluster-configs
key.converter=org.apache.kafka.connect.json.JsonConverter
group.id=connect-cluster
status.storage.topic=connect-cluster-status


security.protocol=SSL
producer.security.protocol=SSL
consumer.security.protocol=SSL
admin.security.protocol=SSL
# TLS / SSL
ssl.truststore.location=/tmp/kafka/cluster.truststore.p12
ssl.truststore.password=[hidden]
ssl.truststore.type=PKCS12

producer.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12
producer.ssl.truststore.password=[hidden]

consumer.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12
consumer.ssl.truststore.password=[hidden]

admin.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12
admin.ssl.truststore.password=[hidden]



OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
2020-07-15 15:58:12,650 INFO WorkerInfo values: 
	jvm.args = -Xms128M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/opt/kafka, -Dlog4j.configuration=file:/opt/kafka/custom-config/log4j.properties
	jvm.spec = Oracle Corporation, OpenJDK 64-Bit Server VM, 1.8.0_252, 25.252-b09
	jvm.classpath = /opt/kafka/bin/../libs/activation-1.1.1.redhat-5.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.5.0.redhat-00002.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.redhat-00003.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.redhat-00002.jar:/opt/kafka/bin/../libs/bcpkix-jdk15on-1.60.0.redhat-00001.jar:/opt/kafka/bin/../libs/bcprov-jdk15on-1.62.0.redhat-00001.jar:/opt/kafka/bin/../libs/commons-cli-1.4.0.redhat-00001.jar:/opt/kafka/bin/../libs/commons-lang-2.6.0.redhat-7.jar:/opt/kafka/bin/../libs/commons-lang3-3.9.0.redhat-00001.jar:/opt/kafka/bin/../libs/connect-api-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/connect-file-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/connect-json-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/connect-mirror-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/connect-mirror-client-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/connect-runtime-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/connect-transforms-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/cruise-control-metrics-reporter-2.0.103.redhat-00002.jar:/opt/kafka/bin/../libs/gson-2.8.2.redhat-5.jar:/opt/kafka/bin/../libs/hk2-api-2.5.0.redhat-00002.jar:/opt/kafka/bin/../libs/hk2-locator-2.5.0.redhat-00002.jar:/opt/kafka/bin/../libs/hk2-utils-2.5.0.redhat-00002.jar:/opt/kafka/bin/../libs/jackson-annotations-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-core-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-databind-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-dataformat-csv-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-datatype-jdk8-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-module-paranamer-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jackson-module-scala_2.12-2.10.4.redhat-00001.jar:/opt/kafka/bin/../libs/jaeger-client-1.1.0.redhat-00002.jar:/opt/kafka/bin/../libs/jaeger-core-1.1.0.redhat-00002.jar:/opt/kafka/bin/../libs/jaeger-thrift-1.1.0.redhat-00002.jar:/opt/kafka/bin/../libs/jaeger-tracerresolver-1.1.0.redhat-00002.jar:/opt/kafka/bin/../libs/jakarta.activation-api-1.2.1.redhat-00002.jar:/opt/kafka/bin/../libs/jakarta.annotation-api-1.3.4.redhat-00002.jar:/opt/kafka/bin/../libs/jakarta.inject-2.5.0.redhat-00002.jar:/opt/kafka/bin/../libs/jakarta.ws.rs-api-2.1.5.redhat-00001.jar:/opt/kafka/bin/../libs/jakarta.xml.bind-api-2.3.2.redhat-00001.jar:/opt/kafka/bin/../libs/javassist-3.22.0.GA-redhat-1.jar:/opt/kafka/bin/../libs/javax.annotation-api-1.3.2.redhat-00001.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.redhat-1.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.1.redhat-00002.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.redhat-00003.jar:/opt/kafka/bin/../libs/jersey-client-2.28.0.redhat-00001.jar:/opt/kafka/bin/../libs/jersey-common-2.28.0.redhat-00001.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.28.0.redhat-00001.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.28.0.redhat-00001.jar:/opt/kafka/bin/../libs/jersey-hk2-2.28.0.redhat-00001.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.28.0.redhat-00001.jar:/opt/kafka/bin/../libs/jersey-server-2.28.0.redhat-00001.jar:/opt/kafka/bin/../libs/jetty-client-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-http-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-io-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-security-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-server-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jetty-util-9.4.26.v20200117-redhat-00001.jar:/opt/kafka/bin/../libs/jmx_prometheus_javaagent-0.12.0.redhat-00001.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.redhat-00002.jar:/opt/kafka/bin/../libs/json-smart-1.1.1.redhat-00001.jar:/opt/kafka/bin/../libs/jsonevent-layout-1.7.0.redhat-00002.jar:/opt/kafka/bin/../libs/kafka-agent-0.18.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka-clients-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka-oauth-client-0.5.0.redhat-00001.jar:/opt/kafka/bin/../libs/kafka-oauth-common-0.5.0.redhat-00001.jar:/opt/kafka/bin/../libs/kafka-oauth-keycloak-authorizer-0.5.0.redhat-00001.jar:/opt/kafka/bin/../libs/kafka-oauth-server-0.5.0.redhat-00001.jar:/opt/kafka/bin/../libs/kafka-streams-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.12-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka-tools-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/kafka_2.12-2.5.0.redhat-00003.jar:/opt/kafka/bin/../libs/keycloak-common-9.0.3.redhat-00002.jar:/opt/kafka/bin/../libs/keycloak-core-9.0.3.redhat-00002.jar:/opt/kafka/bin/../libs/libthrift-0.13.0.redhat-00002.jar:/opt/kafka/bin/../libs/log4j-1.2.17.redhat-3.jar:/opt/kafka/bin/../libs/lz4-java-1.7.1.redhat-00001.jar:/opt/kafka/bin/../libs/maven-artifact-3.6.0.redhat-00001.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.redhat-00003.jar:/opt/kafka/bin/../libs/mirror-maker-agent-0.18.0.redhat-00003.jar:/opt/kafka/bin/../libs/netty-buffer-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/netty-codec-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/netty-common-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/netty-handler-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/netty-resolver-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/netty-transport-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/netty-transport-native-epoll-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/netty-transport-native-unix-common-4.1.48.Final-redhat-00001.jar:/opt/kafka/bin/../libs/okhttp-3.14.4.redhat-00001.jar:/opt/kafka/bin/../libs/okio-1.17.2.redhat-00002.jar:/opt/kafka/bin/../libs/opentracing-api-0.33.0.redhat-00001.jar:/opt/kafka/bin/../libs/opentracing-kafka-client-0.1.12.redhat-00001.jar:/opt/kafka/bin/../libs/opentracing-noop-0.33.0.redhat-00001.jar:/opt/kafka/bin/../libs/opentracing-tracerresolver-0.1.8.redhat-00001.jar:/opt/kafka/bin/../libs/opentracing-util-0.33.0.redhat-00001.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.3.redhat-00001.jar:/opt/kafka/bin/../libs/paranamer-2.8.0.redhat-00001.jar:/opt/kafka/bin/../libs/plexus-utils-3.2.0.redhat-00001.jar:/opt/kafka/bin/../libs/reflections-0.9.12.redhat-00001.jar:/opt/kafka/bin/../libs/rocksdbjni-5.18.3.redhat-00002.jar:/opt/kafka/bin/../libs/scala-collection-compat_2.12-2.1.2.redhat-00001.jar:/opt/kafka/bin/../libs/scala-java8-compat_2.12-0.9.0.redhat-00001.jar:/opt/kafka/bin/../libs/scala-library-2.12.10.redhat-00003.jar:/opt/kafka/bin/../libs/scala-logging_2.12-3.9.0.redhat-00008.jar:/opt/kafka/bin/../libs/scala-reflect-2.12.10.redhat-00003.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.25.redhat-00001.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.25.redhat-00001.jar:/opt/kafka/bin/../libs/snappy-java-1.1.7.2-redhat-00002.jar:/opt/kafka/bin/../libs/tracing-agent-0.18.0.redhat-00003.jar:/opt/kafka/bin/../libs/validation-api-2.0.1.Final-redhat-1.jar:/opt/kafka/bin/../libs/zookeeper-3.5.8.redhat-00001.jar:/opt/kafka/bin/../libs/zookeeper-jute-3.5.8.redhat-00001.jar:/opt/kafka/bin/../libs/zstd-jni-1.4.3.1-redhat-00002.jar
	os.spec = Linux, amd64, 4.18.0-147.20.1.el8_1.x86_64
	os.vcpus = 1
 (org.apache.kafka.connect.runtime.WorkerInfo) [main]
2020-07-15 15:58:12,657 INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed) [main]
2020-07-15 15:58:12,704 INFO Loading plugin from: /tmp/kafka-plugins/confluentinc-connector-jdbc (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,115 INFO Registered loader: PluginClassLoader{pluginLocation=file:/tmp/kafka-plugins/confluentinc-connector-jdbc/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,115 INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,115 INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,115 INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,115 INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,115 INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,129 INFO Loading plugin from: /tmp/kafka-plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,594 INFO Registered loader: PluginClassLoader{pluginLocation=file:/tmp/kafka-plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,594 INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,594 INFO Added plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,594 INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,594 INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,594 INFO Added plugin 'io.debezium.transforms.UnwrapFromEnvelope' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:13,594 INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,836 INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,837 INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,838 INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,838 INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,838 INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,838 INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,838 INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,838 INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,839 INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,840 INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,841 INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,841 INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,841 INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,841 INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,841 INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,841 INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,841 INFO Added alias 'UnwrapFromEnvelope' to plugin 'io.debezium.transforms.UnwrapFromEnvelope' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,842 INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,842 INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,843 INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,843 INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,843 INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,843 INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,843 INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,843 INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
2020-07-15 15:58:16,889 INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	client.dns.lookup = default
	client.id = 
	config.providers = []
	config.storage.replication.factor = 3
	config.storage.topic = connect-cluster-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 3
	offset.storage.topic = connect-cluster-offsets
	plugin.path = [/tmp/kafka-plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	rest.advertised.host.name = 10.129.2.184
	rest.advertised.listener = null
	rest.advertised.port = 8083
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = SSL
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	status.storage.partitions = 5
	status.storage.replication.factor = 3
	status.storage.topic = connect-cluster-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig) [main]
2020-07-15 15:58:16,892 INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils) [main]
2020-07-15 15:58:16,895 INFO AdminClientConfig values: 
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
 (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,155 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,155 WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,155 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,155 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'ssl.truststore.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,156 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [main]
2020-07-15 15:58:22,157 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:22,157 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:22,157 INFO Kafka startTimeMs: 1594828702156 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:22,720 INFO Kafka cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.connect.util.ConnectUtils) [main]
2020-07-15 15:58:22,742 INFO Logging initialized @10772ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log) [main]
2020-07-15 15:58:22,796 INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:22,796 INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:22,803 INFO jetty-9.4.26.v20200117-redhat-00001; built: 2020-01-20T09:58:01.187Z; git: 8d77dd2a20c8beffb38905b7a84e4246220a2486; jvm 1.8.0_252-b09 (org.eclipse.jetty.server.Server) [main]
2020-07-15 15:58:22,829 INFO Started http_8083@4fdf8f12{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector) [main]
2020-07-15 15:58:22,830 INFO Started @10860ms (org.eclipse.jetty.server.Server) [main]
2020-07-15 15:58:22,860 INFO Advertised URI: http://10.129.2.184:8083/ (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:22,860 INFO REST server listening at http://10.129.2.184:8083/, advertising URL http://10.129.2.184:8083/ (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:22,861 INFO Advertised URI: http://10.129.2.184:8083/ (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:22,861 INFO REST admin endpoints at http://10.129.2.184:8083/ (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:22,861 INFO Advertised URI: http://10.129.2.184:8083/ (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:22,867 INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy) [main]
2020-07-15 15:58:22,879 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:22,879 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:22,879 INFO Kafka startTimeMs: 1594828702879 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:23,053 INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig) [main]
2020-07-15 15:58:23,054 INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig) [main]
2020-07-15 15:58:23,309 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:23,309 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:23,309 INFO Kafka startTimeMs: 1594828703309 (org.apache.kafka.common.utils.AppInfoParser) [main]
2020-07-15 15:58:23,312 INFO Kafka Connect distributed worker initialization took 10655ms (org.apache.kafka.connect.cli.ConnectDistributed) [main]
2020-07-15 15:58:23,312 INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect) [main]
2020-07-15 15:58:23,314 INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:23,314 INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,314 INFO Worker starting (org.apache.kafka.connect.runtime.Worker) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,314 INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,314 INFO Starting KafkaBasedLog with topic connect-cluster-offsets (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,315 INFO AdminClientConfig values: 
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
 (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,378 INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:23,421 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'ssl.truststore.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,422 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,423 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,423 INFO Kafka startTimeMs: 1594828703422 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,479 INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session) [main]
2020-07-15 15:58:23,479 INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session) [main]
2020-07-15 15:58:23,481 INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session) [main]
2020-07-15 15:58:23,529 INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,654 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,655 INFO Kafka startTimeMs: 1594828703655 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,664 INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,688 INFO [Producer clientId=producer-1] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [kafka-producer-network-thread | producer-1]
2020-07-15 15:58:23,808 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,808 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,809 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,809 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,809 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,809 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,809 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,809 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,809 INFO Kafka startTimeMs: 1594828703809 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,851 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,913 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Subscribed to partition(s): connect-cluster-offsets-0, connect-cluster-offsets-5, connect-cluster-offsets-10, connect-cluster-offsets-20, connect-cluster-offsets-15, connect-cluster-offsets-9, connect-cluster-offsets-11, connect-cluster-offsets-16, connect-cluster-offsets-4, connect-cluster-offsets-17, connect-cluster-offsets-3, connect-cluster-offsets-24, connect-cluster-offsets-23, connect-cluster-offsets-13, connect-cluster-offsets-18, connect-cluster-offsets-22, connect-cluster-offsets-8, connect-cluster-offsets-2, connect-cluster-offsets-12, connect-cluster-offsets-19, connect-cluster-offsets-14, connect-cluster-offsets-1, connect-cluster-offsets-6, connect-cluster-offsets-7, connect-cluster-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,917 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,917 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,917 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,917 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,917 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:23,918 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
Jul 15, 2020 3:58:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 15, 2020 3:58:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 15, 2020 3:58:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 15, 2020 3:58:23 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2020-07-15 15:58:24,045 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-23 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,046 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-20 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,046 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-5 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,046 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-11 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,046 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-8 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-14 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-17 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-6 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-21 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-24 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-9 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-15 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-12 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-18 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,047 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-22 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-7 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-10 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-13 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-19 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-16 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,049 INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-cluster-offsets-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,141 INFO Finished reading KafkaBasedLog for topic connect-cluster-offsets (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,142 INFO Started KafkaBasedLog for topic connect-cluster-offsets (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,142 INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,145 INFO Worker started (org.apache.kafka.connect.runtime.Worker) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,145 INFO Starting KafkaBasedLog with topic connect-cluster-status (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,145 INFO AdminClientConfig values: 
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
 (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
Jul 15, 2020 3:58:24 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2020-07-15 15:58:24,245 INFO Started o.e.j.s.ServletContextHandler@1dbb650b{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler) [main]
2020-07-15 15:58:24,245 INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer) [main]
2020-07-15 15:58:24,245 INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect) [main]
2020-07-15 15:58:24,276 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,276 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'ssl.truststore.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,277 INFO Kafka startTimeMs: 1594828704277 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,330 INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,409 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,410 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,410 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,410 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,410 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,410 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,410 INFO Kafka startTimeMs: 1594828704410 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,410 INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,435 INFO [Producer clientId=producer-2] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [kafka-producer-network-thread | producer-2]
2020-07-15 15:58:24,502 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,503 INFO Kafka startTimeMs: 1594828704503 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,529 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,557 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Subscribed to partition(s): connect-cluster-status-0, connect-cluster-status-4, connect-cluster-status-1, connect-cluster-status-2, connect-cluster-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,557 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,557 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,557 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,557 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,557 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,636 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-cluster-status-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,637 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-cluster-status-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,637 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-cluster-status-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,637 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-cluster-status-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,639 INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-cluster-status-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,685 INFO Finished reading KafkaBasedLog for topic connect-cluster-status (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,685 INFO Started KafkaBasedLog for topic connect-cluster-status (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,686 INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,686 INFO Starting KafkaBasedLog with topic connect-cluster-configs (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,687 INFO AdminClientConfig values: 
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
 (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'ssl.truststore.type' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,768 INFO Kafka startTimeMs: 1594828704768 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,816 INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,912 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,913 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,913 INFO Kafka startTimeMs: 1594828704912 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,913 INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:24,934 INFO [Producer clientId=producer-3] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [kafka-producer-network-thread | producer-3]
2020-07-15 15:58:25,004 WARN The configuration 'producer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'admin.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'consumer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'producer.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'consumer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'admin.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'consumer.ssl.truststore.password' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'producer.security.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,004 WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,005 WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,005 WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,005 WARN The configuration 'admin.ssl.truststore.location' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,005 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,005 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,005 INFO Kafka startTimeMs: 1594828705005 (org.apache.kafka.common.utils.AppInfoParser) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,028 INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,053 INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Subscribed to partition(s): connect-cluster-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,053 INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-cluster-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,102 INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Resetting offset for partition connect-cluster-configs-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,109 INFO Finished reading KafkaBasedLog for topic connect-cluster-configs (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,110 INFO Started KafkaBasedLog for topic connect-cluster-configs (org.apache.kafka.connect.util.KafkaBasedLog) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,110 INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,110 INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,138 INFO [Worker clientId=connect-1, groupId=connect-cluster] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,138 INFO [Worker clientId=connect-1, groupId=connect-cluster] Discovered group coordinator my-cluster-kafka-0.my-cluster-kafka-brokers.cdc-sink.svc:9093 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,140 INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,141 INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,167 INFO [Worker clientId=connect-1, groupId=connect-cluster] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:25,167 INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:27,521 INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:27,524 INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-a7404d2d-701d-4c56-a800-389cbc8c91c2', leaderUrl='http://10.129.2.182:8083/', offset=7, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:27,524 WARN [Worker clientId=connect-1, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:27,524 INFO [Worker clientId=connect-1, groupId=connect-cluster] Current config state offset -1 is behind group assignment 7, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:27,620 INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:27,620 INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:27,621 INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,526 INFO [Worker clientId=connect-1, groupId=connect-cluster] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,526 INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,526 INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,539 INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,539 INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-a7404d2d-701d-4c56-a800-389cbc8c91c2', leaderUrl='http://10.129.2.182:8083/', offset=7, connectorIds=[my-sink-connector], taskIds=[my-sink-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,539 INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,541 INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,541 INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,550 INFO Creating task my-sink-connector-0 (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,553 INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	transforms = [unwrap, route]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,553 INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	transforms = [unwrap, route]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,568 INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	transforms = [unwrap, route]
	transforms.route.regex = .+\.([^.]+)
	transforms.route.replacement = $1
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,568 INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	transforms = [unwrap, route]
	transforms.route.regex = .+\.([^.]+)
	transforms.route.replacement = $1
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,569 INFO Creating connector my-sink-connector of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,571 INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,571 INFO Instantiated task my-sink-connector-0 with version null of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,572 INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,572 INFO Instantiated connector my-sink-connector with version 5.5.0 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,572 INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,572 INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,572 INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,573 INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,588 INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState, org.apache.kafka.connect.transforms.RegexRouter} (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,589 INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	topics = []
	topics.regex = my-cluster-source\.dbserver1\.inventory\.(addresses|customers|products)
	transforms = [unwrap, route]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,589 INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	topics = []
	topics.regex = my-cluster-source\.dbserver1\.inventory\.(addresses|customers|products)
	transforms = [unwrap, route]
	transforms.route.regex = .+\.([^.]+)
	transforms.route.replacement = $1
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,592 INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = connector-consumer-my-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-sink-connector
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,616 INFO Finished creating connector my-sink-connector (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,618 INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	topics = []
	topics.regex = my-cluster-source\.dbserver1\.inventory\.(addresses|customers|products)
	transforms = [unwrap, route]
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,619 INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connector
	tasks.max = 1
	topics = []
	topics.regex = my-cluster-source\.dbserver1\.inventory\.(addresses|customers|products)
	transforms = [unwrap, route]
	transforms.route.regex = .+\.([^.]+)
	transforms.route.replacement = $1
	transforms.route.type = class org.apache.kafka.connect.transforms.RegexRouter
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.operation.header = false
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,620 INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector) [StartAndStopExecutor-connect-1-1]
2020-07-15 15:58:30,676 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,676 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,676 INFO Kafka startTimeMs: 1594828710676 (org.apache.kafka.common.utils.AppInfoParser) [StartAndStopExecutor-connect-1-2]
2020-07-15 15:58:30,686 INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:30,689 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Subscribed to pattern: 'my-cluster-source\.dbserver1\.inventory\.(addresses|customers|products)' (org.apache.kafka.clients.consumer.KafkaConsumer) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,691 INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,714 INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.password = [hidden]
	connection.url = jdbc:mysql://mysql:3306/inventory?nullCatalogMeansCurrent=true
	connection.user = mysqluser
	db.timezone = UTC
	delete.enabled = true
	dialect.name = 
	fields.whitelist = []
	insert.mode = upsert
	max.retries = 10
	pk.fields = [id]
	pk.mode = record_key
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,720 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,721 INFO WorkerSinkTask{id=my-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,748 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,751 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Discovered group coordinator my-cluster-kafka-0.my-cluster-kafka-brokers.cdc-sink.svc:9093 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,785 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,789 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:30,789 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:33,796 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Finished assignment for group at generation 3: {connector-consumer-my-sink-connector-0-c2db3e19-4117-4aa9-b20f-86cdb4fed94e=Assignment(partitions=[my-cluster-source.dbserver1.inventory.addresses-0, my-cluster-source.dbserver1.inventory.products-0, my-cluster-source.dbserver1.inventory.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:33,803 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:33,804 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Adding newly assigned partitions: my-cluster-source.dbserver1.inventory.addresses-0, my-cluster-source.dbserver1.inventory.products-0, my-cluster-source.dbserver1.inventory.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:33,812 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Setting offset for partition my-cluster-source.dbserver1.inventory.addresses-0 to the committed offset FetchPosition{offset=21, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[my-cluster-kafka-2.my-cluster-kafka-brokers.cdc-sink.svc:9093 (id: 2 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:33,812 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Setting offset for partition my-cluster-source.dbserver1.inventory.customers-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[my-cluster-kafka-0.my-cluster-kafka-brokers.cdc-sink.svc:9093 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:33,812 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Setting offset for partition my-cluster-source.dbserver1.inventory.products-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[my-cluster-kafka-0.my-cluster-kafka-brokers.cdc-sink.svc:9093 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 15:58:33,853 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 15:58:34,055 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 15:58:34,089 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 15:58:34,110 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 15:58:34,163 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 15:58:34,198 INFO UPSERT records:6 , but no count of the number of rows it affected is available (io.confluent.connect.jdbc.sink.BufferedRecords) [task-thread-my-sink-connector-0]
2020-07-15 15:58:48,546 INFO [Worker clientId=connect-1, groupId=connect-cluster] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:48,546 INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:48,546 INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:48,604 INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [DistributedHerder-connect-1-1]
2020-07-15 15:58:48,604 INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-a678497c-68d6-4b10-a353-97fb9daec79c', leaderUrl='http://10.129.2.184:8083/', offset=7, connectorIds=[my-source-connector, my-sink-connector], taskIds=[my-source-connector-0, my-sink-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:48,604 INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:48,605 INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-source-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,605 INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-source-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,605 INFO Creating task my-source-connector-0 (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,605 INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connector
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,606 INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connector
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,606 INFO Creating connector my-source-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,606 INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connector
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,606 INFO Instantiated connector my-source-connector with version 1.0.3.Final-redhat-00001 of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,607 INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connector
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,608 INFO Finished creating connector my-source-connector (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,609 INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connector
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,609 INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connector
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig) [StartAndStopExecutor-connect-1-3]
2020-07-15 15:58:48,610 INFO TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,611 INFO Instantiated task my-source-connector-0 with version 1.0.3.Final-redhat-00001 of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,611 INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,611 INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,611 INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,611 INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,612 INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,612 INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,614 INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [my-cluster-kafka-bootstrap:9093]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = connector-producer-my-source-connector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = /tmp/kafka/cluster.truststore.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,707 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,707 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,707 INFO Kafka startTimeMs: 1594828728707 (org.apache.kafka.common.utils.AppInfoParser) [StartAndStopExecutor-connect-1-4]
2020-07-15 15:58:48,712 INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder) [DistributedHerder-connect-1-1]
2020-07-15 15:58:48,730 INFO [Producer clientId=connector-producer-my-source-connector-0] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [kafka-producer-network-thread | connector-producer-my-source-connector-0]
2020-07-15 15:58:48,746 INFO Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.user = debezium (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.server.id = 184055 (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.history.kafka.bootstrap.servers = my-cluster-kafka-bootstrap:9092 (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.history.kafka.topic = schema-changes.inventory (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.server.name = dbserver2 (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.port = 3306 (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.hostname = mysql (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,747 INFO    database.password = ******** (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,748 INFO    name = my-source-connector (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:48,748 INFO    database.whitelist = inventory (io.debezium.connector.common.BaseSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,476 INFO KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-source-connector-dbhistory, auto.offset.reset=earliest, session.timeout.ms=10000, bootstrap.servers=my-cluster-kafka-bootstrap:9092, client.id=my-source-connector-dbhistory, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1} (io.debezium.relational.history.KafkaDatabaseHistory) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,476 INFO KafkaDatabaseHistory Producer config: {bootstrap.servers=my-cluster-kafka-bootstrap:9092, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-source-connector-dbhistory, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1} (io.debezium.relational.history.KafkaDatabaseHistory) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,479 INFO ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [my-cluster-kafka-bootstrap:9092]
	buffer.memory = 1048576
	client.dns.lookup = default
	client.id = my-source-connector-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,482 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,483 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,483 INFO Kafka startTimeMs: 1594828729482 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,483 INFO Found existing offset: {ts_sec=1594828116, file=mysql-bin.000003, pos=2675, row=6, server_id=223344, event=2} (io.debezium.connector.mysql.MySqlConnectorTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,488 INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [my-cluster-kafka-bootstrap:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = my-source-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-source-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,491 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,491 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,491 INFO Kafka startTimeMs: 1594828729491 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,492 INFO [Producer clientId=my-source-connector-dbhistory] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [kafka-producer-network-thread | my-source-connector-dbhistory]
2020-07-15 15:58:49,495 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,523 INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [my-cluster-kafka-bootstrap:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = my-source-connector-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-source-connector-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,526 INFO Kafka version: 2.5.0.redhat-00003 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,526 INFO Kafka commitId: f960e3745ec74111 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,526 INFO Kafka startTimeMs: 1594828729526 (org.apache.kafka.common.utils.AppInfoParser) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,526 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Subscribed to topic(s): schema-changes.inventory (org.apache.kafka.clients.consumer.KafkaConsumer) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,529 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Cluster ID: 1rqXtBySSz-Y1JrgaM-xJg (org.apache.kafka.clients.Metadata) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,533 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Discovered group coordinator my-cluster-kafka-0.my-cluster-kafka-brokers.cdc-sink.svc:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,533 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,538 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:49,538 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:52,541 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Finished assignment for group at generation 1: {my-source-connector-dbhistory-9c19a7e7-6828-4019-9a57-643ac373f0b2=Assignment(partitions=[schema-changes.inventory-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:52,550 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:52,550 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Adding newly assigned partitions: schema-changes.inventory-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:52,553 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Found no committed offset for partition schema-changes.inventory-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:52,554 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Resetting offset for partition schema-changes.inventory-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,089 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Revoke previously assigned partitions schema-changes.inventory-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,090 INFO [Consumer clientId=my-source-connector-dbhistory, groupId=my-source-connector-dbhistory] Member my-source-connector-dbhistory-9c19a7e7-6828-4019-9a57-643ac373f0b2 sending LeaveGroup request to coordinator my-cluster-kafka-0.my-cluster-kafka-brokers.cdc-sink.svc:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,113 INFO Step 0: Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnectorTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,116 INFO MySQL has the binlog file 'mysql-bin.000003' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,152 INFO Requested thread factory for connector MySqlConnector, id = dbserver2 named = binlog-client (io.debezium.util.Threads) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,178 INFO Creating thread debezium-mysqlconnector-dbserver2-binlog-client (io.debezium.util.Threads) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,183 INFO Creating thread debezium-mysqlconnector-dbserver2-binlog-client (io.debezium.util.Threads) [blc-mysql:3306]
Jul 15, 2020 3:58:54 PM com.github.shyiko.mysql.binlog.BinaryLogClient connect
INFO: Connected to mysql:3306 at mysql-bin.000003/2675 (sid:184055, cid:590)
2020-07-15 15:58:54,323 INFO Connected to MySQL binlog at mysql:3306, starting at binlog file 'mysql-bin.000003', pos=2675, skipping 2 events plus 6 rows (io.debezium.connector.mysql.BinlogReader) [blc-mysql:3306]
2020-07-15 15:58:54,323 INFO WorkerSourceTask{id=my-source-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask) [task-thread-my-source-connector-0]
2020-07-15 15:58:54,324 INFO Creating thread debezium-mysqlconnector-dbserver2-binlog-client (io.debezium.util.Threads) [blc-mysql:3306]
2020-07-15 15:59:30,682 INFO WorkerSinkTask{id=my-sink-connector-0} Committing offsets asynchronously using sequence number 1: {my-cluster-source.dbserver1.inventory.addresses-0=OffsetAndMetadata{offset=21, leaderEpoch=null, metadata=''}, my-cluster-source.dbserver1.inventory.products-0=OffsetAndMetadata{offset=9, leaderEpoch=null, metadata=''}, my-cluster-source.dbserver1.inventory.customers-0=OffsetAndMetadata{offset=19, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 15:59:48,712 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 15:59:48,713 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 15:59:48,777 INFO WorkerSourceTask{id=my-source-connector-0} Finished commitOffsets successfully in 65 ms (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:00:17,363 INFO UPSERT records:1 , but no count of the number of rows it affected is available (io.confluent.connect.jdbc.sink.BufferedRecords) [task-thread-my-sink-connector-0]
2020-07-15 16:00:17,633 INFO 3 records sent during previous 00:01:23.454, last recorded offset: {ts_sec=1594828817, file=mysql-bin.000003, pos=3470, row=1, server_id=223344, event=2} (io.debezium.connector.mysql.BinlogReader) [task-thread-my-source-connector-0]
2020-07-15 16:00:30,684 INFO WorkerSinkTask{id=my-sink-connector-0} Committing offsets asynchronously using sequence number 2: {my-cluster-source.dbserver1.inventory.addresses-0=OffsetAndMetadata{offset=21, leaderEpoch=null, metadata=''}, my-cluster-source.dbserver1.inventory.products-0=OffsetAndMetadata{offset=9, leaderEpoch=null, metadata=''}, my-cluster-source.dbserver1.inventory.customers-0=OffsetAndMetadata{offset=21, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:00:48,778 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:00:48,778 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:00:48,784 INFO WorkerSourceTask{id=my-source-connector-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:01:48,784 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:01:48,784 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:02:48,785 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:02:48,785 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:02:53,407 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:02:53,410 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:02:53,417 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:02:53,631 INFO 2 records sent during previous 00:02:36.0, last recorded offset: {ts_sec=1594828973, file=mysql-bin.000003, pos=4088, row=1, server_id=223344, event=2} (io.debezium.connector.mysql.BinlogReader) [task-thread-my-source-connector-0]
2020-07-15 16:02:53,864 WARN Write of 2 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:02:53,866 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:02:53,869 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:02:53,869 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:02:56,869 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,897 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,926 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,928 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,936 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,936 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,938 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,945 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,949 WARN Write of 2 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:02:56,950 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,953 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:02:56,953 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:02:59,954 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:02:59,982 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,008 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,010 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,016 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,017 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,019 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,026 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,030 WARN Write of 2 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:00,030 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,033 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:00,033 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:03,034 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,062 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,089 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,091 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,098 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,099 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,101 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,107 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,111 WARN Write of 2 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:03,111 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,114 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:03,114 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:06,115 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,141 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,167 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,169 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,175 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,176 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,178 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,184 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,187 WARN Write of 2 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:06,188 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,191 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:06,191 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:09,192 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,219 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,247 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,249 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,255 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,255 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,257 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,262 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,266 WARN Write of 2 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:09,267 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,270 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:09,270 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:12,270 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,298 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,334 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,337 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,342 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,343 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,345 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,350 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,354 WARN Write of 2 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:12,354 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,357 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:12,357 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:15,358 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,394 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,422 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,423 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,429 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,429 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,431 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,436 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,440 WARN Write of 2 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:15,440 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,443 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:15,443 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:18,444 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,470 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,500 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,502 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,507 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,508 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,510 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,515 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,520 WARN Write of 2 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:18,520 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,524 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:18,524 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:21,524 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,553 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,582 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,584 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,590 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,591 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,593 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,598 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,601 WARN Write of 2 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:21,602 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,605 INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:21,605 ERROR WorkerSinkTask{id=my-sink-connector-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:93)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:24,605 INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,631 INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,657 INFO Checking MySql dialect for existence of table "addresses" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,658 INFO Using MySql dialect table "addresses" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,664 INFO Setting metadata for table "addresses" to Table{name='"addresses"', columns=[Column{'zip', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'customer_id', isPrimaryKey=false, allowsNull=false, sqlType=INT}, Column{'type', isPrimaryKey=false, allowsNull=false, sqlType=ENUM}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'state', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'city', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'street', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,664 INFO Checking MySql dialect for existence of table "customers" (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,666 INFO Using MySql dialect table "customers" present (io.confluent.connect.jdbc.dialect.MySqlDatabaseDialect) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,672 INFO Setting metadata for table "customers" to Table{name='"customers"', columns=[Column{'last_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'email', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'first_name', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,675 WARN Write of 2 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:855)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:437)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:814)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:216)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:182)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:72)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:74)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1094)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:834)
	... 17 more
2020-07-15 16:03:24,676 ERROR WorkerSinkTask{id=my-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
 (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:24,676 INFO WorkerSinkTask{id=my-sink-connector-0} Committing offsets synchronously using sequence number 5: {my-cluster-source.dbserver1.inventory.addresses-0=OffsetAndMetadata{offset=22, leaderEpoch=null, metadata=''}, my-cluster-source.dbserver1.inventory.products-0=OffsetAndMetadata{offset=9, leaderEpoch=null, metadata=''}, my-cluster-source.dbserver1.inventory.customers-0=OffsetAndMetadata{offset=22, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,683 ERROR WorkerSinkTask{id=my-sink-connector-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask) [task-thread-my-sink-connector-0]
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:568)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:326)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:228)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:196)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:184)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:546)
	... 10 more
Caused by: java.sql.SQLException: java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))
java.sql.SQLIntegrityConstraintViolationException: Cannot add or update a child row: a foreign key constraint fails (`inventory`.`addresses`, CONSTRAINT `addresses_ibfk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`id`))

	... 12 more
2020-07-15 16:03:24,683 ERROR WorkerSinkTask{id=my-sink-connector-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,683 INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,684 INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,686 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Revoke previously assigned partitions my-cluster-source.dbserver1.inventory.addresses-0, my-cluster-source.dbserver1.inventory.products-0, my-cluster-source.dbserver1.inventory.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 16:03:24,686 INFO [Consumer clientId=connector-consumer-my-sink-connector-0, groupId=connect-my-sink-connector] Member connector-consumer-my-sink-connector-0-c2db3e19-4117-4aa9-b20f-86cdb4fed94e sending LeaveGroup request to coordinator my-cluster-kafka-0.my-cluster-kafka-brokers.cdc-sink.svc:9093 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator) [task-thread-my-sink-connector-0]
2020-07-15 16:03:48,785 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:03:48,786 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:03:48,791 INFO WorkerSourceTask{id=my-source-connector-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:04:48,791 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:04:48,791 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:05:48,791 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:05:48,791 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:06:48,792 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:06:48,792 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:07:48,793 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:07:48,793 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:08:48,793 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:08:48,793 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:09:48,793 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:09:48,793 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:10:48,794 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:10:48,794 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:11:48,794 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:11:48,794 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:12:48,794 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:12:48,794 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:13:48,795 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:13:48,795 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:14:48,795 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:14:48,795 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:15:48,795 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:15:48,796 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:16:48,796 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:16:48,796 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:17:48,796 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:17:48,796 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:18:48,796 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:18:48,797 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:19:48,797 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:19:48,797 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:20:48,797 INFO WorkerSourceTask{id=my-source-connector-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
2020-07-15 16:20:48,797 INFO WorkerSourceTask{id=my-source-connector-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask) [SourceTaskOffsetCommitter-1]
